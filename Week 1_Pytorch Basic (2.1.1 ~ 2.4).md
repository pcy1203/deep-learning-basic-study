### ğŸ“Œ 2.1. íŒŒì´í† ì¹˜ ê°œìš”

- **íŒŒì´í† ì¹˜(PyTorch)** : GPUì—ì„œ í…ì„œ ì¡°ì‘ ë° ë™ì  ì‹ ê²½ë§ êµ¬ì¶•ì´ ê°€ëŠ¥í•œ í”„ë ˆì„ì›Œí¬ (ìœ ì—°ì„±ê³¼ ì†ë„ë¥¼ ì œê³µí•˜ëŠ” ë”¥ëŸ¬ë‹ ë¼ì´ë¸ŒëŸ¬ë¦¬)
    - PyTorchì˜ ë°ì´í„° í˜•íƒœëŠ” **í…ì„œ(Tensor)** = ë‹¨ì¼ ë°ì´í„° í˜•ì‹ì˜ ë‹¤ì°¨ì› í–‰ë ¬
        - `torch.tensor()` ì‚¬ìš©
        - 1ì°¨ì› ë°°ì—´ í˜•íƒœ(ìŠ¤í† ë¦¬ì§€) ì €ì¥, ìŠ¤íŠ¸ë¼ì´ë“œ = ê° ì°¨ì›ì—ì„œ ë‹¤ìŒ ìš”ì†Œë¥¼ ì–»ê¸° ìœ„í•´ ê±´ë„ˆë›°ì–´ì•¼ í•˜ëŠ” ìš”ì†Œ ê°œìˆ˜
    - `.cuda()`ë¥¼ ì‚¬ìš©í•˜ì—¬ GPUë¡œ ì—°ì‚°ì„ ë¹ ë¥´ê²Œ ìˆ˜í–‰í•  ìˆ˜ ìˆë„ë¡ í•¨
    - í›ˆë ¨ì„ ë°˜ë³µí•  ë•Œë§ˆë‹¤ ëª¨ë¸ì˜ ë„¤íŠ¸ì›Œí¬ ì¡°ì‘ì´ ê°€ëŠ¥í•œ ë™ì  ì‹ ê²½ë§
- **íŒŒì´í† ì¹˜ ì•„í‚¤í…ì²˜** : íŒŒì´í† ì¹˜ API (ì‚¬ìš©ì ì‚¬ìš©) - íŒŒì´í† ì¹˜ ì—”ì§„ (ë‹¤ì°¨ì› í…ì„œ ë° ìë™ ë¯¸ë¶„ ì²˜ë¦¬) - ì—°ì‚° ì²˜ë¦¬
    - íŒŒì´í† ì¹˜ API : `torch` (GPU ì§€ì› í…ì„œ íŒ¨í‚¤ì§€), `torch.autograd` (ìë™ ë¯¸ë¶„ íŒ¨í‚¤ì§€), `torch.nn` (ì‹ ê²½ë§ êµ¬ì¶• ë° í›ˆë ¨ íŒ¨í‚¤ì§€), `torch.multiprocessing` (íŒŒì´ì¬ ë©€í‹°í”„ë¡œì„¸ì‹± íŒ¨í‚¤ì§€), `torch.utils` (DataLoader ë° ê¸°íƒ€ ìœ í‹¸ë¦¬í‹° íŒ¨í‚¤ì§€)
    - íŒŒì´í† ì¹˜ ì—”ì§„ : Autograd C++ (ë¯¸ë¶„ ìë™ ê³„ì‚°), Aten C++ (C++ í…ì„œ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì œê³µ), JIT C++ (ê³„ì‚° ìµœì í™” JIT ì»´íŒŒì¼ëŸ¬)
    - ì—°ì‚° ì²˜ë¦¬ : ë‹¤ì°¨ì› í…ì„œ ì—°ì‚° ì²˜ë¦¬ (C ë˜ëŠ” CUDA íŒ¨í‚¤ì§€)

### ğŸ“Œ 2.2. íŒŒì´í† ì¹˜ ê¸°ë³¸ ë¬¸ë²•

ë°ì´í„°ì…‹ ë¡œë“œ â†’ ëª¨ë¸ ì •ì˜ (íŒŒë¼ë¯¸í„° ì •ì˜) â†’ ëª¨ë¸ í›ˆë ¨ â†’ ëª¨ë¸ í‰ê°€

- **í…ì„œ ì‚¬ìš©ë²•**
    - `torch.tensor(ë°°ì—´)` : í…ì„œ ìƒì„±
        - `torch.tensor(ë°°ì—´, device="cuda:0", dtype=torch.float64)`
        - `í…ì„œ.numpy()` ë¡œ ndarray ë³€í™˜ ê°€ëŠ¥
        - `torch.FloatTensor`, `torch.DoubleTensor`, `torch.LongTensor`
    - `í…ì„œ[ì¸ë±ìŠ¤]` í˜¹ì€ `í…ì„œ[ì¸ë±ìŠ¤:ì¸ë±ìŠ¤]` : ì¸ë±ìŠ¤ ì¡°ì‘
    - í…ì„œ ê°„ì˜ ì‚¬ì¹™ ì—°ì‚° ê°€ëŠ¥ (ë‹¨, íƒ€ì…ì´ ë‹¤ë¥´ë©´ ë¶ˆê°€)
    - `í…ì„œ.view(ê°’, ...)` : M x â€¦ x N í–‰ë ¬ë¡œ ì°¨ì› ì¡°ì‘
        - `-1` ì§€ì • ì‹œ ë‹¤ë¥¸ ì°¨ì›ìœ¼ë¡œë¶€í„° ê°’ì„ ìœ ì¶”
    - ì´ì™¸ : `stack` `cat` `t` `transpose`
- **ë°ì´í„°ì…‹ ë¡œë“œ**
    - ë‹¨ìˆœí•˜ê²Œ íŒŒì¼ ë¶ˆëŸ¬ì˜¤ê¸° : `pd.read_csv('íŒŒì¼ëª….csv')`
        - `x = torch.from_numpy(data['x'].values).unsqueeze(dim=1).float()`
    - ì»¤ìŠ¤í…€ ë°ì´í„°ì…‹(ë°ì´í„°ë¥¼ ì¡°ê¸ˆì”© ë‚˜ëˆ„ì–´ ë¶ˆëŸ¬ì˜¤ê¸°) :
        
        ```python
        import pandas as pd
        import torch
        from torch.utils.data import Dataset
        from torch.utils.data import DataLoader
        
        class CustomDataset(Dataset):
            def __init__(self, csv_file):
                self.label = pd.read_csv(csv_file)
        
            def __len__(self):
                return len(self.label)
        
            def __getitem__(self, idx):
                sample = torch.tensor(self.label.iloc[idx,0:3]).int()
                label = torch.tensor(self.label.iloc[idx,3]).int()
                return sample, label
        
        tensor_dataset = CustomDataset('íŒŒì¼ëª….csv')
        dataset = DataLoader(tensor_dataset, batch_size=4, shuffle=True)
        ```
        
    - í† ì¹˜ë¹„ì „(torchvision) : ë°ì´í„°ì…‹ íŒ¨í‚¤ì§€ (requests ë¼ì´ë¸ŒëŸ¬ë¦¬ ë³„ë„ ì„¤ì¹˜)
        
        ```python
        import torchvision.transforms as transforms
        
        mnist_transform = transforms.Compose([
            transforms.ToTensor(),
            transforms.Normalize((0.5,), (1.0,))
        ]) # í‰ê· ì´ 0.5, í‘œì¤€í¸ì°¨ê°€ 1.0ì´ ë˜ë„ë¡ ë°ì´í„°ì˜ ë¶„í¬(normalize)ë¥¼ ì¡°ì •
        
        from torchvision.datasets import MNIST
        import requests
        download_root = 'ê²½ë¡œ'
        
        train_dataset = MNIST(download_root, transform=mnist_transform, train=True, download=True)
        valid_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)
        test_dataset = MNIST(download_root, transform=mnist_transform, train=False, download=True)
        ```
        
- **ëª¨ë¸ ì •ì˜**
    - ë‹¨ìˆœ ì‹ ê²½ë§ : `nn.Linear(in_features=1, out_features=1, bias=True)`
    - `nn.Module()` ìƒì†
        
        ```python
        class MLP(Module):
            def __init__(self, inputs):
                super(MLP, self).__init__()
                self.layer = Linear(inputs, 1)
                self.activation = Sigmoid()
        
            def forward(self, X):
                X = self.layer(X)
                X = self.activation(X)
                return X
        ```
        
    - Sequential ì‹ ê²½ë§ ì •ì˜
        - `model.modules()` ë„¤íŠ¸ì›Œí¬ ëª¨ë“  ë…¸ë“œ ë°˜í™˜, `model.children()` ê°™ì€ ìˆ˜ì¤€ í•˜ìœ„ ë…¸ë“œ ë°˜í™˜
        
        ```python
        import torch.nn as nn
        class MLP(nn.Module):
            def __init__(self):
                super(MLP, self).__init__()
                self.layer1 = nn.Sequential(
                    nn.Conv2d(in_channels=3, out_channels=64, kernel_size=5),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2)
                )
        
                self.layer2 = nn.Sequential(
                    nn.Conv2d(in_channels=64, out_channels=30, kernel_size=5),
                    nn.ReLU(inplace=True),
                    nn.MaxPool2d(2)
                )
        
                self.layer3 = nn.Sequential(
                    nn.Linear(in_features=30*5*5, out_features=10, bias=True),
                    nn.ReLU(inplace=True)
                )
        
                def forward(self, x):
                    x = self.layer1(x)
                    x = self.layer2(x)
                    x = x.view(x.shape[0], -1)
                    x = self.layer3(x)
                    return x
        model = MLP()
        ```
        
    - í•¨ìˆ˜ë¡œ ì‹ ê²½ë§ ì •ì˜
        
        ```python
        def MLP(in_features=1, hidden_features=20, out_features=1):
            hidden = nn.Linear(in_features=in_features, out_features=hidden_features, bias=True)
            activation = nn.ReLU()
            output = nn.Linear(in_features=hidden_features, out_features=out_features, bias=True)
            net = nn.Sequential(hidden, activation, output)
            return net
        ```
        
- **ëª¨ë¸ íŒŒë¼ë¯¸í„° ì •ì˜**
    - ì†ì‹¤ í•¨ìˆ˜(loss function) : ì¶œë ¥(wx + b)ê³¼ ì‹¤ì œ ê°’(ì •ë‹µ)(y) ì‚¬ì´ì˜ ì˜¤ì°¨
        - ì´ì§„ ë¶„ë¥˜ BCELoss, ë‹¤ì¤‘ ë¶„ë¥˜ CrossEntropyLoss, íšŒê·€ ëª¨ë¸ MSELoss
    - ì˜µí‹°ë§ˆì´ì €(optimizer) : ëª¨ë¸ ì—…ë°ì´íŠ¸ ë°©ë²• ê²°ì • (step() ë©”ì„œë“œë¥¼ í†µí•´ ì „ë‹¬ë°›ì€ íŒŒë¼ë¯¸í„° ì—…ë°ì´íŠ¸)
        - `torch.optim.Optimizer(params, defaults)` ê¸°ë³¸ í´ë˜ìŠ¤
        - `zero_grad()` ë©”ì„œë“œ : íŒŒë¼ë¯¸í„° ê¸°ìš¸ê¸°ë¥¼ 0ìœ¼ë¡œ ë§Œë“¦
        - Adadelta, Adagrad, Adam, SparseAdam, Adamax, ASGD, LBFGS, RMSProp, Rprop, SGD
    - í•™ìŠµë¥  ìŠ¤ì¼€ì¤„ëŸ¬(learning rate scheduler) : ì§€ì •í•œ ì—í¬í¬ë¥¼ ì§€ë‚  ë•Œë§ˆë‹¤ í•™ìŠµë¥  ê°ì†Œ (ì´ˆê¸° ë¹ ë¥¸ í•™ìŠµ, ì „ì—­ ìµœì†Œì  ê·¼ì²˜ì—ì„œ ìµœì ì ì„ ì°¾ì„ ìˆ˜ ìˆë„ë¡ í•¨)
        - LambdaLR, StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau
    - ì§€í‘œ(metrics) : í›ˆë ¨ê³¼ í…ŒìŠ¤íŠ¸ ë‹¨ê³„ ëª¨ë‹ˆí„°ë§
    - ì˜ˆì‹œ
        
        ```python
        from torch.optim import optimizer
        criterion = torch.nn.MSELoss()
        optimizer = torch.optim.SGD(model.parameters(), lr=0.01, momentum=0.9)
        scheduler = torch.optim.lr_scheduler.LambdaLR(optimizer=optimizer, lr_lambda=lambda epoch: 0.95 ** epoch)
        for epoch in range(1, 100+1): 
            for x, y in dataloader:
                optimizer.zero_grad()
        loss_fn(model(x), y).backward()
        optimizer.step()
        scheduler.step()
        ```
        
- **ëª¨ë¸ í›ˆë ¨**
    - ëª¨ë¸, ì†ì‹¤ í•¨ìˆ˜, ì˜µí‹°ë§ˆì´ì € ì •ì˜
    - `optimizer.zero_grad()` ê¸°ìš¸ê¸° ì´ˆê¸°í™” (ëˆ„ì ì´ í•„ìš”í•˜ì§€ ì•Šì€ ê²½ìš°, ë°°ì¹˜ê°€ ë°˜ë³µë  ë•Œë§ˆë‹¤)
    - `output = model(input)`  ì¶œë ¥ ê³„ì‚°, `loss = loss_fn(output, target)` ì˜¤ì°¨ ê³„ì‚°
    - `loss.backward()` ê¸°ìš¸ê¸° ê°’ ê³„ì‚°(ì—­ì „íŒŒ í•™ìŠµ)
    - `optimizer.step()` ê¸°ìš¸ê¸° ì—…ë°ì´íŠ¸
- **ëª¨ë¸ í‰ê°€**
    - í•¨ìˆ˜ ì´ìš© (torchmetrics ì„¤ì¹˜) : `torchmetrics.functional.accuracy(pred, target)`
    - ëª¨ë“ˆ ì´ìš©
        
        ```python
        import torch
        import torchmetrics
        metric = torchmetrics.Accuracy()
        
        n_batches = 10
        for i in range(n_batches):
            preds = torch.randn(10, 5).softmax(dim=-1)
            target = torch.randint(5, (10,))
        
            acc = metric(preds, target)
            print(f"Accuracy on batch {i}: {acc}")
        
        acc = metric.compute()
        print(f"Accuracy on all data: {acc}")
        ```
        
- **í›ˆë ¨ ê³¼ì • ëª¨ë‹ˆí„°ë§**
    - í…ì„œë³´ë“œ ì„¤ì •, ê¸°ë¡, ëª¨ë¸ êµ¬ì¡° ì‚´í´ë³´ê¸° (tensorboard ì„¤ì¹˜)
        - `tensorboard --logdir=<ì €ì¥ ìœ„ì¹˜> --port=6006`
        - `model.train()` ëª¨ë¸ í›ˆë ¨(í›ˆë ¨ ë°ì´í„°ì…‹ ì‚¬ìš©), ë“œë¡­ì•„ì›ƒ í™œì„±í™”, `model.eval()` ëª¨ë¸ í‰ê°€(ê²€ì¦ê³¼ í…ŒìŠ¤íŠ¸ ë°ì´í„°ì…‹ ì‚¬ìš©), ëª¨ë“  ë…¸ë“œ ì‚¬ìš©, ì—­ì „íŒŒ ë¶ˆí•„ìš”
    
    ```python
    import torch
    from torch.utils.tensorboard import SummaryWriter
    writer = SummaryWriter("ì €ì¥ ìœ„ì¹˜")
    
    for epoch in range(num_epochs):
        model.train()  # í•™ìŠµ ëª¨ë“œë¡œ ì „í™˜(dropout=True)
        batch_loss = 0.0
    
        for i, (x, y) in enumerate(dataloader):
            x, y = x.to(device).float(), y.to(device).float()
            outputs = model(x)
            loss = criterion(outputs, y)
            writer.add_scalar("Loss", loss, epoch)
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
    
    writer.close()
    
    # ----------------------------------------------
    
    model.eval() # ê²€ì¦ ëª¨ë“œë¡œ ì „í™˜(dropout=False)
    with torch.no_grad():
        valid_loss = 0
    
        for x, y in valid_dataloader:
            outputs = model(x)
            loss = F.cross_entropy(outputs, y.long().squeeze())
            valid_loss += float(loss)
            y_hat += [outputs]
    
    valid_loss = valid_loss / len(valid_loader)
    ```
    

### ğŸ“Œ 2.3. ì‹¤ìŠµ í™˜ê²½ ì„¤ì •

- **ì‹¤ìŠµ í™˜ê²½ êµ¬ì¶•**
    - ì•„ë‚˜ì½˜ë‹¤ ì„¤ì¹˜ https://www.anaconda.com/download
    - `conda create -n <ê°€ìƒí™˜ê²½ ì´ë¦„> python=3.9.0` : ê°€ìƒí™˜ê²½ ìƒì„±
    - `conda env list` : ìƒì„±ëœ ê°€ìƒí™˜ê²½ í™•ì¸
    - `activate <ê°€ìƒí™˜ê²½ ì´ë¦„>` : ê°€ìƒí™˜ê²½ í™œì„±í™”
    - `conda env remove -n <ê°€ìƒí™˜ê²½ ì´ë¦„>` : ê°€ìƒí™˜ê²½ ì‚­ì œ
    - `python -m ipykernel install --user --name <ê°€ìƒí™˜ê²½ ì´ë¦„> --display-name "<ê°€ìƒí™˜ê²½ ì´ë¦„>"` : ê°€ìƒí™˜ê²½ì— ì»¤ë„ ì—°ê²°
    - `conda install pytorch=1.9.0 torchvision=0.10.0 torchaudio=0.9.0 -c pytorch` : Pytorch ì„¤ì¹˜
    - `jupyter notebook` : ì£¼í”¼í„° ë…¸íŠ¸ë¶ ì‹¤í–‰

### ğŸ“Œ 2.4. íŒŒì´í† ì¹˜ ì½”ë“œ ë§›ë³´ê¸°

- **íŒŒì´í† ì¹˜ ì½”ë“œ ë§›ë³´ê¸°**
    - matplotlib (ê·¸ë˜í”„), seaborn (ì‹œê°í™”), scikit-learn (ë¨¸ì‹ ëŸ¬ë‹) ì„¤ì¹˜
    - ë°ì´í„° íŒŒì•…í•˜ê¸°, ë°ì´í„° ì „ì²˜ë¦¬(preprocessing)
        - ë²”ì£¼í˜• ë°ì´í„° â†’ dataset(category) â†’ ë„˜íŒŒì´ ë°°ì—´ â†’ í…ì„œ
        - `astype()` ë©”ì„œë“œë¡œ ë°ì´í„°ë¥¼ ë²”ì£¼í˜•ìœ¼ë¡œ ì „í™˜
        - `cat.codes()` ë²”ì£¼í˜• ë°ì´í„°ë¥¼ ìˆ«ì(ë„˜íŒŒì´ ë°°ì—´)ë¡œ ë³€í™˜
        - `np.stack` ìƒˆë¡œìš´ ì¶•ìœ¼ë¡œ í•©ì¹˜ê¸°, `np.concatenate` ì¶• ê¸°ì¤€ ì—°ê²°
        - `get_dummies()` ë„˜íŒŒì´ ë°°ì—´ë¡œ ë³€í™˜ (ê°€ë³€ìˆ˜ë¡œ ë§Œë“¦)
        - `ravel()` `reshape()` `flatten()` í…ì„œì˜ ì°¨ì› ë°”ê¾¸ê¸°
    - ì›Œë“œ ì„ë² ë”© : ìœ ì‚¬í•œ ë‹¨ì–´ë¼ë¦¬ ìœ ì‚¬í•˜ê²Œ ì¸ì½”ë”©ë˜ë„ë¡ í‘œí˜„í•˜ëŠ” ë°©ë²• (ì„ë² ë”© í¬ê¸° ì •ì˜ í•„ìš”, ì¹¼ëŸ¼ ê³ ìœ  ê°’ ìˆ˜ / 2 ë§ì´ ì‚¬ìš©)
    - ëª¨ë¸ì˜ ë„¤íŠ¸ì›Œí¬ ê³„ì¸µ
        - Linear : ì„ í˜• ê³„ì¸µ, ì„ í˜• ë³€í™˜ ì§„í–‰ (y = Wx + b)
        - ReLU : í™œì„±í™” í•¨ìˆ˜
        - BatchNorm1d : ë°°ì¹˜ ì •ê·œí™”
        - Dropout : ê³¼ì í•© ë°©ì§€
- **ë”¥ëŸ¬ë‹ ë¶„ë¥˜ ëª¨ë¸ì˜ ì„±ëŠ¥ í‰ê°€ ì§€í‘œ**
    - TP (ì‹¤ì œ == ì˜ˆì¸¡ == True), TN (ì‹¤ì œ == ì˜ˆì¸¡ == False), FP (ì‹¤ì œ False, ì˜ˆì¸¡ True, Type I ì˜¤ë¥˜), FN (ì‹¤ì œ True, ì˜ˆì¸¡ False, Type II ì˜¤ë¥˜)
    - ì •í™•ë„(accuracy) = TP + TN / TP + TN + FP + FN
    - ì¬í˜„ìœ¨(recall) = TP / TP + FN (ì •ë‹µ Trueì¼ ë•Œ ì˜ˆì¸¡ True)
    - ì •ë°€ë„(precision) = TP / TP + FP (ì˜ˆì¸¡ Trueì¼ ë•Œ ì‹¤ì œ True)
    - F1-Score : 2 x Precision x Recall / (Precision + Recall) â†’ ì¡°í™”í‰ê· 

### **ğŸ¤” ë” ì•Œì•„ë³´ê¸°**

[ì˜µí‹°ë§ˆì´ì €(Optimizer)ì™€ í•™ìŠµë¥ (Learning Rate)](https://naver.me/GpCEzizb)

[[pytorch] Learning Rate Scheduler (í•™ìŠµë¥  ë™ì  ë³€ê²½)](https://naver.me/IgJDDd94)

[ê²½ì‚¬ í•˜ê°•ë²• Gradient Descent ì— ëŒ€í•œ ìˆ˜í•™ì  ì´í•´ì™€ í™œìš©](https://data-scientist-jeong.tistory.com/46)