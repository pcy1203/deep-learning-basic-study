{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "ë”¥ëŸ¬ë‹ ê¸°ì´ˆ ìŠ¤í„°ë”” / ë°•ì°¬ì˜"
      ],
      "metadata": {
        "id": "QnOdxq5EtRWi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# ğŸ“Œ 10.1. ì„ë² ë”©"
      ],
      "metadata": {
        "id": "hs133cipRlat"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "â˜‘ï¸ ****10.1.1. í¬ì†Œ í‘œí˜„ ê¸°ë°˜ ì„ë² ë”©****"
      ],
      "metadata": {
        "id": "qVfiYBcTj8xw"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JTnuhEnqRgWL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4a01c81-d627-466a-bc0f-7986d681b00e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n",
            "[2 2 1 0 1 0]\n",
            "[[0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]]\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")\n",
        "\n",
        "import os\n",
        "os.chdir(\"drive/MyDrive/2025-1 í”„ë¡œë©”í…Œìš°ìŠ¤ ìŠ¤í„°ë””\")\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "class2 = pd.read_csv(\"chap10/data/class2.csv\")\n",
        "\n",
        "from sklearn import preprocessing\n",
        "label_encoder = preprocessing.LabelEncoder()  # ë°ì´í„° ì¸ì½”ë”©\n",
        "onehot_encoder = preprocessing.OneHotEncoder()  # ë°ì´í„°ë¥¼ ìˆ«ì í˜•ì‹ìœ¼ë¡œ í‘œí˜„\n",
        "\n",
        "train_x = label_encoder.fit_transform(class2['class2'])\n",
        "print(train_x)\n",
        "train_x = onehot_encoder.fit_transform(np.array(class2['class2']).reshape(-1, 1)).toarray()\n",
        "print(train_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "â˜‘ï¸ **10.1.2. íšŸìˆ˜ ê¸°ë°˜ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "LCNRLrc7HD7m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "corpus = [\n",
        "    'This is last chance.',\n",
        "    'and if you do not have this chance.',\n",
        "    'you will never get any chance.',\n",
        "    'will you do get this one?',\n",
        "    'please, get this chance',\n",
        "]\n",
        "\n",
        "vect = CountVectorizer()\n",
        "vect.fit(corpus)\n",
        "print(vect.vocabulary_)\n",
        "\n",
        "vect.transform(['you will never get any chance.']).toarray()"
      ],
      "metadata": {
        "id": "ufylBIzTGgV8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d576194-3fdb-4a74-af20-36c82f073266"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'this': 13, 'is': 7, 'last': 8, 'chance': 2, 'and': 0, 'if': 6, 'you': 15, 'do': 3, 'not': 10, 'have': 5, 'will': 14, 'never': 9, 'get': 4, 'any': 1, 'one': 11, 'please': 12}\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0, 1, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, 0, 1, 1]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vect = CountVectorizer(stop_words=[\"and\", \"is\", \"please\", \"this\"]).fit(corpus)\n",
        "print(vect.vocabulary_)"
      ],
      "metadata": {
        "id": "oiWWdHZpHJYR",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5310fa49-95aa-41f4-ec45-19bdd6aa0199"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'last': 6, 'chance': 1, 'if': 5, 'you': 11, 'do': 2, 'not': 8, 'have': 4, 'will': 10, 'never': 7, 'get': 3, 'any': 0, 'one': 9}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "doc = ['I like machine learning', 'I love deep learning', 'I run everyday']\n",
        "\n",
        "tfidf_vectorizer = TfidfVectorizer(min_df=1)\n",
        "tfidf_matrix = tfidf_vectorizer.fit_transform(doc)\n",
        "print(tfidf_matrix.toarray())\n",
        "print()\n",
        "\n",
        "doc_distance = (tfidf_matrix * tfidf_matrix.T)\n",
        "print('ìœ ì‚¬ë„ë¥¼ ìœ„í•œ', str(doc_distance.get_shape()[0]), 'x', str(doc_distance.get_shape()[1]), 'í–‰ë ¬ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.')\n",
        "print(doc_distance.toarray())"
      ],
      "metadata": {
        "id": "JnGP0XrvJFBf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bce6b0e1-cc11-4be7-be84-a28cf0c77b8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.         0.         0.4736296  0.62276601 0.         0.62276601\n",
            "  0.        ]\n",
            " [0.62276601 0.         0.4736296  0.         0.62276601 0.\n",
            "  0.        ]\n",
            " [0.         0.70710678 0.         0.         0.         0.\n",
            "  0.70710678]]\n",
            "\n",
            "ìœ ì‚¬ë„ë¥¼ ìœ„í•œ 3 x 3 í–‰ë ¬ì„ ë§Œë“¤ì—ˆìŠµë‹ˆë‹¤.\n",
            "[[1.       0.224325 0.      ]\n",
            " [0.224325 1.       0.      ]\n",
            " [0.       0.       1.      ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "â˜‘ï¸ **10.1.3. ì˜ˆì¸¡ ê¸°ë°˜ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "m1td0pEJMmBN"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gensim"
      ],
      "metadata": {
        "id": "XeMlI_ecM37E",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe9b3f7e-4244-409f-91f0-91165adc16d3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting gensim\n",
            "  Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (8.1 kB)\n",
            "Collecting numpy<2.0,>=1.18.5 (from gensim)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy<1.14.0,>=1.7.0 (from gensim)\n",
            "  Downloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m60.6/60.6 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: smart-open>=1.8.1 in /usr/local/lib/python3.11/dist-packages (from gensim) (7.1.0)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.11/dist-packages (from smart-open>=1.8.1->gensim) (1.17.2)\n",
            "Downloading gensim-4.3.3-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (26.7 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m26.7/26.7 MB\u001b[0m \u001b[31m87.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m118.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading scipy-1.13.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, scipy, gensim\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "  Attempting uninstall: scipy\n",
            "    Found existing installation: scipy 1.15.2\n",
            "    Uninstalling scipy-1.15.2:\n",
            "      Successfully uninstalled scipy-1.15.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed gensim-4.3.3 numpy-1.26.4 scipy-1.13.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt_tab')"
      ],
      "metadata": {
        "id": "XKBtdMoFOW2U",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aba6a23a-dec0-48e3-c2f8-462c79ad355e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt_tab.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.tokenize import sent_tokenize, word_tokenize\n",
        "import warnings\n",
        "warnings.filterwarnings(action='ignore')\n",
        "import gensim\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "sample = open(\"chap10/data/peter.txt\", \"r\", encoding='UTF8')  # ë°ì´í„°ì…‹ ë¡œë”©\n",
        "s = sample.read()\n",
        "\n",
        "f = s.replace(\"\\n\", \" \")\n",
        "data = []\n",
        "\n",
        "for i in sent_tokenize(f):  # ë¬¸ì¥ë§ˆë‹¤ ë°˜ë³µ\n",
        "    temp = []\n",
        "    for j in word_tokenize(i):  # ë¬¸ì¥ì„ ë‹¨ì–´ë¡œ í† í°í™”\n",
        "        temp.append(j.lower())  # í† í°í™”ëœ ë‹¨ì–´ë¥¼ ì†Œë¬¸ìë¡œ ë³€í™˜í•˜ì—¬ tempì— ì €ì¥\n",
        "    data.append(temp)\n",
        "\n",
        "print(data)"
      ],
      "metadata": {
        "id": "0YnOk-7hHSvj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f0e6485-7b56-4b4e-a3f8-b041852e537b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[['once', 'upon', 'a', 'time', 'in', 'london', ',', 'the', 'darlings', 'went', 'out', 'to', 'a', 'dinner', 'party', 'leaving', 'their', 'three', 'children', 'wendy', ',', 'jhon', ',', 'and', 'michael', 'at', 'home', '.'], ['after', 'wendy', 'had', 'tucked', 'her', 'younger', 'brothers', 'jhon', 'and', 'michael', 'to', 'bed', ',', 'she', 'went', 'to', 'read', 'a', 'book', '.'], ['she', 'heard', 'a', 'boy', 'sobbing', 'outside', 'her', 'window', '.'], ['he', 'was', 'flying', '.'], ['there', 'was', 'little', 'fairy', 'fluttering', 'around', 'him', '.'], ['wendy', 'opened', 'the', 'window', 'to', 'talk', 'to', 'him', '.'], ['â€œ', 'hello', '!'], ['who', 'are', 'you', '?'], ['why', 'are', 'you', 'crying', 'â€', ',', 'wendy', 'asked', 'him', '.'], ['â€œ', 'my', 'name', 'is', 'peter', 'pan', '.'], ['my', 'shadow', 'wouldn', 'â€™', 't', 'stock', 'to', 'me.', 'â€', ',', 'he', 'replied', '.'], ['she', 'asked', 'him', 'to', 'come', 'in', '.'], ['peter', 'agreed', 'and', 'came', 'inside', 'the', 'room', '.'], ['wendy', 'took', 'his', 'shadow', 'and', 'sewed', 'it', 'to', 'his', 'shoe', 'tips', '.'], ['now', 'his', 'shadow', 'followed', 'him', 'wherever', 'peter', 'pan', 'went', '!'], ['he', 'was', 'delighted', 'and', 'asked', 'wendy', 'â€œ', 'why', 'don', 'â€™', 't', 'you', 'come', 'with', 'me', 'to', 'my', 'home', '.'], ['the', 'neverland', '.'], ['i', 'lived', 'there', 'with', 'my', 'fairy', 'tinker', 'bell.', 'â€', 'wendy', '?'], ['â€œ', 'oh', '!'], ['what', 'a', 'wonderful', 'idea', '!'], ['let', 'me', 'wake', 'up', 'john', 'and', 'micheal', 'too', '.'], ['could', 'you', 'teach', 'us', 'how', 'to', 'fly', '?', 'â€', '.'], ['â€œ', 'yes', '!'], ['of', 'course', '!'], ['get', 'them', 'we', 'will', 'all', 'fly', 'together.', 'â€', 'peter', 'pan', 'replied', 'and', 'so', 'it', 'was', '.'], ['five', 'little', 'figures', 'flew', 'out', 'of', 'the', 'window', 'of', 'the', 'darlings', 'and', 'headed', 'towards', 'neverland', '.'], ['as', 'they', 'flew', 'over', 'the', 'island', ',', 'peter', 'pan', 'told', 'the', 'children', 'more', 'about', 'his', 'homeland', '.'], ['â€œ', 'all', 'the', 'children', 'who', 'get', 'lost', 'come', 'and', 'stay', 'with', 'tinker', 'bell', 'and', 'me', ',', 'â€', 'peter', 'told', 'them', '.'], ['the', 'indians', 'also', 'live', 'in', 'neverland', '.'], ['the', 'mermaids', 'live', 'in', 'the', 'lagoon', 'around', 'the', 'island', '.'], ['and', 'a', 'very', 'mean', 'pirate', 'called', 'captain', 'hook', 'keeps', 'troubling', 'everyone', '.'], ['â€œ', 'crocodile', 'bit', 'his', 'one', 'arm', '.'], ['so', 'the', 'captain', 'had', 'to', 'put', 'a', 'hook', 'in', 'its', 'place', '.'], ['since', 'then', 'he', 'is', 'afraid', 'of', 'crocodiles', '.'], ['and', 'rightly', 'so', '!'], ['if', 'the', 'crocodile', 'ever', 'found', 'captain', 'hook', 'it', 'will', 'eat', 'up', 'the', 'rest', 'of', 'it', 'couldn', 'â€™', 't', 'eat', 'last', 'time.', 'â€', 'peter', 'told', 'them', '.'], ['soon', 'they', 'landed', 'on', 'the', 'island', '.'], ['and', 'to', 'the', 'surprise', 'of', 'wendy', ',', 'jhon', 'and', 'michael', ',', 'peter', 'pan', 'let', 'them', 'in', 'through', 'a', 'small', 'opening', 'in', 'a', 'tree', '.'], ['inside', 'the', 'tree', 'was', 'a', 'large', 'room', 'with', 'children', 'inside', 'it', '.'], ['somewhere', 'huddled', 'by', 'the', 'fire', 'in', 'the', 'corner', 'and', 'somewhere', 'playing', 'amongst', 'themselves', '.'], ['their', 'faces', 'lit', 'up', 'when', 'they', 'saw', 'peter', 'pan', ',', 'tinker', 'bell', ',', 'and', 'their', 'guests', '.'], ['â€œ', 'hello', 'everyone', '.'], ['this', 'is', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['they', 'will', 'be', 'staying', 'with', 'us', 'from', 'now', 'on.', 'â€', 'peter', 'pan', 'introduced', 'them', 'to', 'all', 'children', '.'], ['children', 'welcomed', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['a', 'few', 'days', 'passed', '.'], ['and', 'they', 'settled', 'into', 'a', 'routine', '.'], ['wendy', 'would', 'take', 'care', 'of', 'all', 'the', 'children', 'in', 'the', 'day', 'and', 'would', 'go', 'out', 'with', 'peter', 'pan', 'and', 'her', 'brothers', 'in', 'the', 'evening', 'to', 'learn', 'about', 'the', 'island', '.'], ['she', 'would', 'cook', 'for', 'them', 'and', 'stitch', 'new', 'clothes', 'for', 'them', '.'], ['he', 'even', 'made', 'a', 'lovely', 'new', 'dress', 'for', 'tinker', 'bell', '.'], ['one', 'evening', ',', 'as', 'they', 'were', 'out', 'exploring', 'the', 'island', 'peter', 'pan', 'warned', 'everyone', 'and', 'said', ',', 'â€œ', 'hide', '!'], ['hide', '!'], ['pirates', '!'], ['and', 'they', 'have', 'kidnapped', 'the', 'indian', 'princess', 'tiger', 'lily', '.'], ['they', 'have', 'kept', 'her', 'there', ',', 'tied', 'up', 'by', 'the', 'rocks', ',', 'near', 'the', 'water.', 'â€', 'peter', 'was', 'afraid', 'and', 'the', 'princess', 'would', 'drown', ',', 'is', 'she', 'fell', 'into', 'the', 'water', '.'], ['so', ',', 'in', 'a', 'voice', 'that', 'sounded', 'like', 'captain', 'hook', ',', 'he', 'shouted', 'instructions', 'to', 'the', 'pirates', 'who', 'guarded', 'her', ',', 'â€œ', 'you', 'fools', '!'], ['let', 'her', 'go', 'at', 'once', '!'], ['do', 'it', 'before', 'i', 'come', 'there', 'or', 'else', 'i', 'will', 'throw', 'each', 'one', 'of', 'you', 'into', 'the', 'water.', 'â€', 'the', 'pirates', 'got', 'scared', 'and', 'immediately', 'released', 'the', 'princes', '.'], ['she', 'quickly', 'dived', 'into', 'the', 'water', 'and', 'swam', 'to', 'the', 'safety', 'of', 'her', 'home', '.'], ['soon', 'everyone', 'found', 'out', 'how', 'peter', 'pan', 'had', 'rescued', 'the', 'princess', '.'], ['when', 'captain', 'hook', 'found', 'out', 'how', 'peter', 'had', 'tricked', 'his', 'men', 'he', 'was', 'furious', '.'], ['and', 'swore', 'to', 'have', 'his', 'revenge', '.'], ['that', 'night', 'wendy', 'told', 'peter', 'pan', ',', 'that', 'she', 'and', 'her', 'brother', 'wanted', 'to', 'go', 'back', 'home', 'since', 'they', 'missed', 'their', 'parents', '.'], ['she', 'said', 'if', 'the', 'lost', 'children', 'could', 'also', 'return', 'to', 'her', 'world', 'they', 'could', 'find', 'a', 'nice', 'home', 'for', 'them', '.'], ['peter', 'pan', 'didn', 'â€™', 't', 'want', 'to', 'leave', 'neverland', '.'], ['but', 'the', 'sake', 'of', 'the', 'lost', 'children', 'he', 'agreed', ',', 'although', 'a', 'bit', 'sadly', '.'], ['he', 'would', 'miss', 'his', 'friends', 'dearly', '.'], ['the', 'next', 'morning', 'all', 'the', 'lost', 'children', 'left', 'with', 'wendy', ',', 'jhon', ',', 'and', 'michael', '.'], ['but', 'on', 'the', 'way', ',', 'captain', 'hook', 'and', 'his', 'men', 'kidnapped', 'all', 'of', 'them', '.'], ['he', 'tied', 'them', 'and', 'kept', 'them', 'on', 'once', 'of', 'his', 'ships', '.'], ['as', 'soon', 'as', 'peter', 'found', 'out', 'about', 'it', 'he', 'rushed', 'to', 'the', 'ship', '.'], ['he', 'swung', 'himself', 'from', 'a', 'tress', 'branch', 'and', 'on', 'to', 'the', 'deck', 'of', 'the', 'ship', 'where', 'all', 'the', 'children', 'were', 'tied', 'up', '.'], ['he', 'swung', 'his', 'sword', 'bravely', 'and', 'threw', 'over', 'the', 'pirates', 'who', 'tried', 'to', 'stop', 'him', '.'], ['quickly', 'he', 'released', 'everyone', 'from', 'their', 'captor', 'â€™', 's', 'ties', '.'], ['wendy', ',', 'jhon', ',', 'michael', 'and', 'tinker', 'bell', 'helped', 'all', 'the', 'children', 'into', 'the', 'water', ',', 'where', 'their', 'friends', 'from', 'the', 'indian', 'camp', 'were', 'ready', 'with', 'smaller', 'boats', 'to', 'take', 'them', 'to', 'safety', 'peter', 'pan', 'now', 'went', 'looking', 'for', 'captain', 'hook', '.'], ['â€œ', 'let', 'us', 'finished', 'this', 'forever', 'mr.', 'hook', 'â€', ',', 'peter', 'challenged', 'captain', 'hook', '.'], ['â€œ', 'yes', '!'], ['peter', 'pan', ',', 'you', 'have', 'caused', 'me', 'enough', 'trouble', '.'], ['it', 'is', 'time', 'that', 'we', 'finished', 'this.', 'â€', 'hook', 'replied', '.'], ['with', 'his', 'sword', 'drawn', ',', 'he', 'raced', 'towards', 'peter', 'pan', '.'], ['quick', 'on', 'his', 'feet', ',', 'peter', 'pan', 'stepped', 'aside', 'and', 'pushed', 'hook', 'inside', 'the', 'sea', 'where', 'the', 'crocodile', 'was', 'waiting', 'to', 'eat', 'the', 'rest', 'of', 'hook', '.'], ['everyone', 'rejoiced', 'as', 'captain', 'hook', 'was', 'out', 'of', 'their', 'lives', 'forever', '.'], ['everybody', 'headed', 'back', 'to', 'london', '.'], ['mr.', 'and', 'mrs', '.'], ['darling', 'was', 'so', 'happy', 'to', 'see', 'their', 'children', 'and', 'they', 'agreed', 'to', 'adopt', 'the', 'lost', 'children', '.'], ['they', 'even', 'asked', 'peter', 'pan', 'to', 'come', 'and', 'live', 'with', 'them', '.'], ['but', 'peter', 'pan', 'said', ',', 'he', 'never', 'wanted', 'to', 'grow', 'up', ',', 'so', 'he', 'and', 'tinker', 'bell', 'will', 'go', 'back', 'to', 'neverland', '.'], ['peter', 'pan', 'promised', 'everyone', 'that', 'he', 'will', 'visit', 'again', 'sometime', '!'], ['and', 'he', 'flew', 'out', 'of', 'the', 'window', 'with', 'tinker', 'bell', 'by', 'his', 'side', '.']]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model1 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=0)\n",
        "\n",
        "print(\"Cosine similarity between 'peter' \" + \"'wendy' - CBOW : \",\n",
        "      model1.wv.similarity('peter', 'wendy'))\n",
        "print(\"Cosine similarity between 'peter' \" + \"'hook' - CBOW : \",\n",
        "      model1.wv.similarity('peter', 'hook'))"
      ],
      "metadata": {
        "id": "1S7mdmqGM2db",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6dbb8123-0baa-4676-a893-4e1b3e5d08ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'peter' 'wendy' - CBOW :  0.074393824\n",
            "Cosine similarity between 'peter' 'hook' - CBOW :  0.027709836\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = gensim.models.Word2Vec(data, min_count=1, vector_size=100, window=5, sg=1)\n",
        "\n",
        "print(\"Cosine similarity between 'peter' \" + \"'wendy' - Skip Gram : \",\n",
        "      model2.wv.similarity('peter', 'wendy'))\n",
        "print(\"Cosine similarity between 'peter' \" + \"'hook' - Skip Gram : \",\n",
        "      model2.wv.similarity('peter', 'hook'))"
      ],
      "metadata": {
        "id": "q-nODCwnQXzn",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a65348a-f636-4a1d-a7cb-b731ce6f4e3a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cosine similarity between 'peter' 'wendy' - Skip Gram :  0.40088683\n",
            "Cosine similarity between 'peter' 'hook' - Skip Gram :  0.5201673\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import FastText\n",
        "\n",
        "model = FastText('chap10/data/peter.txt', vector_size=4, window=3, min_count=1, epochs=10)\n",
        "sim_score = model.wv.similarity('peter', 'wendy')\n",
        "print(sim_score)\n",
        "sim_score = model.wv.similarity('peter', 'hook')\n",
        "print(sim_score)"
      ],
      "metadata": {
        "id": "8CXWm3b3RoWl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b3f27e68-452a-4343-d1ac-9557ed5481ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:gensim.models.word2vec:Each 'sentences' item should be a list of words (usually unicode strings). First item here is instead plain <class 'str'>.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.4592452\n",
            "0.043825686\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from __future__ import print_function\n",
        "from gensim.models import KeyedVectors\n",
        "\n",
        "model_kr = KeyedVectors.load_word2vec_format('chap10/data/wiki.ko.vec')\n",
        "\n",
        "find_similar_to = 'ë…¸ë ¥'\n",
        "\n",
        "for similar_word in model_kr.similar_by_word(find_similar_to):\n",
        "    print(\"Word: {0}, Similarity: {1:.2f}\".format(\n",
        "        similar_word[0], similar_word[1]\n",
        "    ))\n",
        "\n",
        "similarities = model_kr.most_similar(positive=['ë™ë¬¼', 'ìœ¡ì‹ë™ë¬¼'], negative=['ì‚¬ëŒ'])\n",
        "print(similarities)"
      ],
      "metadata": {
        "id": "el-GLwFxm4LM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "â˜‘ï¸ **10.1.4. íšŸìˆ˜/ì˜ˆì¸¡ ê¸°ë°˜ ì„ë² ë”©**"
      ],
      "metadata": {
        "id": "iD9hiKLenmqR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "%matplotlib notebook\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('ggplot')\n",
        "from sklearn.decomposition import PCA\n",
        "from gensim.test.utils import datapath, get_tmpfile\n",
        "from gensim.models import KeyedVectors\n",
        "from gensim.scripts.glove2word2vec import glove2word2vec\n",
        "\n",
        "glove_file = datapath('chap10/data/glove.6B.100d.txt')\n",
        "word2vec_glove_file = get_tmpfile(\"glove.6B.100d.word2vec.txt\")  # ì›Œë“œíˆ¬ë²¡í„° í˜•íƒœë¡œ ë³€í™˜\n",
        "glove2word2vec(glove_file, word2vec_glove_file)\n",
        "\n",
        "model = KeyedVectors.load_word2vec_format(word2vec_glove_file)\n",
        "model.most_similar('bill')  # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "model.most_similar('cherry')  # ê°€ì¥ ìœ ì‚¬í•œ ë‹¨ì–´ë“¤ì˜ ë¦¬ìŠ¤íŠ¸\n",
        "\n",
        "# woman, kingê³¼ ìœ ì‚¬ì„±ì´ ë†’ìœ¼ë©´ì„œ manê³¼ ê´€ë ¨ì„±ì´ ì—†ëŠ” ë‹¨ì–´\n",
        "result = model.most_similar(positive=['woman', 'king'], negative=['man'])\n",
        "print(\"{}: {:.4f}\".format(*result[0]))"
      ],
      "metadata": {
        "id": "bTGqnwMpnWAy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def analogy(x1, x2, y1):\n",
        "    result = model.most_similar(positive=[y1, x2], negative=[x1])\n",
        "    return result[0][0]\n",
        "analogy('australia', 'beer', 'france')\n",
        "analogy('tall', 'tallest', 'long')\n",
        "print(model.doesnt_match(\"breakfast cereal dinner lunch\".split()))  # ìœ ì‚¬ë„ê°€ ê°€ì¥ ë‚®ì€ ë‹¨ì–´ ë°˜í™˜"
      ],
      "metadata": {
        "id": "fS9zZg1Cn0Cc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}